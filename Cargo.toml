[package]
name = "oomllama"
version = "0.6.0"
edition = "2021"
authors = ["Jasper van de Meent <jasper@humotica.com>", "Root AI <root_idd@humotica.nl>"]
description = "OomLlama - Efficient LLM inference with .oom format. 2x smaller than GGUF."
license = "MIT"
repository = "https://github.com/jaspertvdm/oomllama"
keywords = ["llm", "inference", "quantization", "gguf", "oom", "oomllama", "humotica"]
readme = "README.md"

[dependencies]
# Async runtime
tokio = { version = "1", features = ["full"] }

# Serialization
serde = { version = "1", features = ["derive"] }
serde_json = "1"

# Memory mapping for lazy loading
memmap2 = "0.9"

# Error handling
anyhow = "1.0"
thiserror = "2.0.17"

# Candle for GPU inference
candle-core = { version = "0.9.1", features = ["cuda"] }
candle-nn = { version = "0.9.1", features = ["cuda"] }
candle-transformers = { version = "0.9.1", features = ["cuda"] }

# HuggingFace Hub for model downloads
hf-hub = "0.4.3"
tokenizers = "0.22.2"

# CLI
colored = "2.0"
shellexpand = "3.1"
regex = "1.12.2"
tracing = "0.1.44"
tracing-subscriber = "0.3.22"

# Python bindings (optional)
pyo3 = { version = "0.22", features = ["extension-module"], optional = true }

[features]
default = []
python = ["pyo3"]
full = ["python"]

[profile.release]
opt-level = 3
lto = true
codegen-units = 1
panic = "abort"
strip = true

[[bin]]
name = "oomllama"
path = "src/bin/oomllama.rs"

[[bin]]
name = "gguf2oom"
path = "src/bin/quantize.rs"

[lib]
name = "oomllama"
path = "src/lib.rs"
crate-type = ["lib", "cdylib"]
